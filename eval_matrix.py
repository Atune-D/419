#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡å®éªŒçŸ©é˜µè¯„ä¼° - ç³»ç»ŸåŒ–å¯¹æ¯”ä¸åŒé…ç½®

æ”¯æŒï¼š
- å¤šä¸ªåµŒå…¥æ¨¡å‹
- Thread-level vs Turn-level ç²’åº¦
- BM25 æ··åˆæ£€ç´¢
- äº¤å‰ç¼–ç å™¨é‡æ’

Usage:
    # å®Œæ•´å®éªŒçŸ©é˜µ
    python eval_matrix.py \
      --test data/working/threads.test.jsonl \
      --models intfloat/e5-base-v2 BAAI/bge-small-en-v1.5 sentence-transformers/all-MiniLM-L6-v2 \
      --granularities thread turn \
      --k 10 --bm25 --rerank \
      --out report/experiments.csv
    
    # å¿«é€Ÿæµ‹è¯•
    python eval_matrix.py --test data/working/threads.test.jsonl --models intfloat/e5-base-v2
"""

import argparse
import jsonlines
import re
import numpy as np
import csv
import time
from pathlib import Path
from tqdm import tqdm

def tokenize(s):
    """ç®€å•åˆ†è¯"""
    return re.findall(r"[a-z0-9]+", (s or "").lower())

def load_threads(path):
    """åŠ è½½ threads"""
    return list(jsonlines.open(path))

def build_docs(threads, granularity="turn"):
    """
    æ„å»ºæ£€ç´¢æ–‡æ¡£
    
    Args:
        threads: list of thread dicts
        granularity: 'thread' or 'turn'
    
    Returns:
        (doc_ids, doc_texts) tuples
    """
    ids, texts = [], []
    seen_texts = set()  # å»é‡
    
    if granularity == "thread":
        # Thread-level: æ¯ä¸ªthreadä¸€ä¸ªæ–‡æ¡£
        for th in threads:
            # SCSA summary
            scsa_parts = []
            for t in th.get("turns", []):
                sc = t.get("scsa")
                if isinstance(sc, str) and sc.strip():
                    scsa_parts.append(sc)
            scsa = "\n".join(scsa_parts)
            
            # RAW content (å‰4ä¸ªturns)
            raw_parts = []
            for t in th.get("turns", [])[:4]:
                role = t.get("role", "").upper()
                subj = t.get("subject", "")
                body = t.get("body", "")
                raw_parts.append(f"[{role}] {subj}\n{body}")
            raw = "\n\n".join(raw_parts)
            
            text = (scsa + "\n" + raw).strip()
            
            # å»é‡
            if text and text not in seen_texts:
                ids.append(th["thread_id"])
                texts.append(text)
                seen_texts.add(text)
    
    else:  # turn-level
        # Turn-level: æ¯ä¸ªturnä¸€ä¸ªæ–‡æ¡£
        for th in threads:
            for i, t in enumerate(th.get("turns", [])):
                role = t.get("role", "").upper()
                subj = t.get("subject", "")
                body = t.get("body", "")
                scsa = t.get("scsa", "") if isinstance(t.get("scsa"), str) else ""
                
                text = (scsa + "\n" + f"[{role}] {subj}\n{body}").strip()
                
                # å»é‡
                if text and text not in seen_texts:
                    ids.append(f"{th['thread_id']}#t{i}")
                    texts.append(text)
                    seen_texts.add(text)
    
    return ids, texts

def last_customer_query(th):
    """æå–æœ€åä¸€æ¡å®¢æˆ·é‚®ä»¶ä½œä¸ºæŸ¥è¯¢"""
    for t in reversed(th.get("turns", [])):
        if t.get("role") == "customer":
            # ä¼˜å…ˆSCSAï¼Œå¦åˆ™ç”¨body
            scsa = t.get("scsa")
            if isinstance(scsa, str) and scsa.strip():
                return scsa
            body = t.get("body", "")
            if body.strip():
                return body
    return None

def extract_thread_id(doc_id):
    """ä»æ–‡æ¡£IDæå–thread_id"""
    return doc_id.split("#")[0]

def run_experiment(test_threads, model_name, granularity, k, use_bm25, use_rerank):
    """
    è¿è¡Œå•æ¬¡å®éªŒ
    
    Returns:
        dict with metrics
    """
    from sentence_transformers import SentenceTransformer
    import faiss
    
    # æ„å»ºæ–‡æ¡£
    doc_ids, doc_texts = build_docs(test_threads, granularity=granularity)
    
    if len(doc_texts) == 0:
        return {
            "model": model_name,
            "granularity": granularity,
            "bm25": int(use_bm25),
            "rerank": int(use_rerank),
            "k": k,
            "recall": 0.0,
            "mrr": 0.0,
            "sec": 0.0,
            "queries": 0,
            "docs": 0,
            "error": "No documents built"
        }
    
    # åŠ è½½åµŒå…¥æ¨¡å‹
    print(f"  ğŸ“¦ Loading model: {model_name}")
    emb = SentenceTransformer(model_name)
    
    # ç¼–ç æ–‡æ¡£
    print(f"  ğŸ”¨ Encoding {len(doc_texts)} documents...")
    E = emb.encode(
        doc_texts, 
        batch_size=256, 
        normalize_embeddings=True, 
        convert_to_numpy=True, 
        show_progress_bar=False
    )
    
    # æ„å»ºFAISSç´¢å¼•
    index = faiss.IndexFlatIP(E.shape[1])
    index.add(E)
    
    # BM25 (å¯é€‰)
    bm25 = None
    if use_bm25:
        try:
            from rank_bm25 import BM25Okapi
            print(f"  ğŸ“Š Building BM25 index...")
            corpus = [tokenize(t) for t in doc_texts]
            bm25 = BM25Okapi(corpus)
        except ImportError:
            print(f"  âš ï¸  rank_bm25 not installed, skipping BM25")
            use_bm25 = False
    
    # Reranker (å¯é€‰)
    reranker = None
    if use_rerank:
        try:
            from sentence_transformers import CrossEncoder
            print(f"  ğŸ¯ Loading reranker...")
            reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
        except Exception as e:
            print(f"  âš ï¸  Reranker load failed: {e}, skipping rerank")
            use_rerank = False
    
    # è¯„ä¼°
    total = 0
    hit = 0
    rr_sum = 0.0
    
    t0 = time.time()
    
    print(f"  ğŸ” Evaluating on {len(test_threads)} threads...")
    for th in tqdm(test_threads, desc="  Queries", leave=False):
        q = last_customer_query(th)
        if not q:
            continue
        
        # ç¼–ç æŸ¥è¯¢
        vq = emb.encode([q], normalize_embeddings=True, convert_to_numpy=True)
        
        # å‘é‡æ£€ç´¢
        retrieve_k = 100 if use_rerank else k
        D, I = index.search(vq, min(retrieve_k, len(doc_texts)))
        
        # å€™é€‰é›†
        candidates = []
        for j, idx in enumerate(I[0]):
            doc_id = doc_ids[idx]
            doc_text = doc_texts[idx]
            score = float(D[0][j])
            candidates.append((doc_id, doc_text, score))
        
        # BM25 èåˆ (å¯é€‰)
        if bm25 and use_bm25:
            q_tokens = tokenize(q)
            bm25_scores = bm25.get_scores(q_tokens)
            # å½’ä¸€åŒ–
            if bm25_scores.max() > 0:
                bm25_scores = bm25_scores / bm25_scores.max()
            
            # èåˆ: 70% vector + 30% BM25
            fused_candidates = []
            for doc_id, doc_text, vec_score in candidates:
                try:
                    # æ‰¾åˆ°å¯¹åº”çš„BM25åˆ†æ•°
                    doc_idx = doc_ids.index(doc_id)
                    bm25_score = float(bm25_scores[doc_idx])
                    fused_score = 0.7 * vec_score + 0.3 * bm25_score
                    fused_candidates.append((doc_id, doc_text, fused_score))
                except (ValueError, IndexError):
                    fused_candidates.append((doc_id, doc_text, vec_score))
            
            candidates = sorted(fused_candidates, key=lambda x: x[2], reverse=True)
        
        # é‡æ’ (å¯é€‰)
        if reranker and use_rerank:
            pairs = [(q, text) for _, text, _ in candidates[:100]]
            rerank_scores = reranker.predict(pairs)
            reranked = sorted(
                zip(candidates[:100], rerank_scores), 
                key=lambda x: x[1], 
                reverse=True
            )
            top_ids = [doc_id for (doc_id, _, _), _ in reranked[:k]]
        else:
            top_ids = [doc_id for doc_id, _, _ in candidates[:k]]
        
        # è¯„ä¼°
        total += 1
        gt = th["thread_id"]
        
        # æå–thread_idsï¼ˆå¤„ç†turn-levelçš„æƒ…å†µï¼‰
        retrieved_thread_ids = [extract_thread_id(doc_id) for doc_id in top_ids]
        
        if gt in retrieved_thread_ids:
            hit += 1
            # MRR: ç¬¬ä¸€æ¬¡å‡ºç°çš„ä½ç½®
            rank = retrieved_thread_ids.index(gt) + 1
            rr_sum += 1.0 / rank
    
    elapsed = time.time() - t0
    
    recall_k = hit / total if total > 0 else 0.0
    mrr_k = rr_sum / total if total > 0 else 0.0
    
    return {
        "model": model_name,
        "granularity": granularity,
        "bm25": int(use_bm25),
        "rerank": int(use_rerank),
        "k": k,
        "recall": round(recall_k, 3),
        "mrr": round(mrr_k, 3),
        "sec": round(elapsed, 1),
        "queries": total,
        "docs": len(doc_texts),
    }

def main():
    parser = argparse.ArgumentParser(description="Batch retrieval evaluation with experiment matrix")
    parser.add_argument("--test", required=True, help="Test JSONL file")
    parser.add_argument("--models", nargs="+", required=True, 
                       help="Embedding models (e.g., intfloat/e5-base-v2)")
    parser.add_argument("--granularities", nargs="+", default=["thread", "turn"],
                       help="Document granularity: thread, turn")
    parser.add_argument("--k", type=int, default=10, help="K for Recall@K and MRR@K")
    parser.add_argument("--bm25", action="store_true", help="Enable BM25 hybrid retrieval")
    parser.add_argument("--rerank", action="store_true", help="Enable cross-encoder reranking")
    parser.add_argument("--out", default="report/experiments.csv", help="Output CSV file")
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(args.out).parent.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½æµ‹è¯•é›†
    print(f"\n{'='*70}")
    print(f"ğŸ“Š BATCH EVALUATION - Experiment Matrix")
    print(f"{'='*70}")
    print(f"Test file:     {args.test}")
    print(f"Models:        {', '.join(args.models)}")
    print(f"Granularities: {', '.join(args.granularities)}")
    print(f"K:             {args.k}")
    print(f"BM25:          {args.bm25}")
    print(f"Rerank:        {args.rerank}")
    print(f"Output:        {args.out}")
    print(f"{'='*70}\n")
    
    test_threads = load_threads(args.test)
    print(f"âœ… Loaded {len(test_threads)} test threads\n")
    
    # è¿è¡Œå®éªŒçŸ©é˜µ
    results = []
    total_experiments = len(args.models) * len(args.granularities)
    
    for i, model_name in enumerate(args.models, 1):
        for j, gran in enumerate(args.granularities, 1):
            exp_num = (i - 1) * len(args.granularities) + j
            print(f"\n{'#'*70}")
            print(f"Experiment {exp_num}/{total_experiments}: {model_name} | {gran}")
            print(f"{'#'*70}")
            
            try:
                result = run_experiment(
                    test_threads, 
                    model_name, 
                    gran, 
                    args.k, 
                    args.bm25, 
                    args.rerank
                )
                results.append(result)
                
                # æ‰“å°ç»“æœ
                status = "âœ…" if result["recall"] >= 0.80 and result["mrr"] >= 0.50 else "âŒ"
                print(f"\n  {status} Results:")
                print(f"     Recall@{args.k}: {result['recall']:.3f}")
                print(f"     MRR@{args.k}:    {result['mrr']:.3f}")
                print(f"     Time:      {result['sec']:.1f}s")
                print(f"     Queries:   {result['queries']}")
                
            except Exception as e:
                print(f"  âŒ Error: {e}")
                results.append({
                    "model": model_name,
                    "granularity": gran,
                    "bm25": int(args.bm25),
                    "rerank": int(args.rerank),
                    "k": args.k,
                    "recall": 0.0,
                    "mrr": 0.0,
                    "sec": 0.0,
                    "queries": 0,
                    "docs": 0,
                    "error": str(e)
                })
    
    # ä¿å­˜ç»“æœ
    if results:
        with open(args.out, "w", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=list(results[0].keys()))
            writer.writeheader()
            writer.writerows(results)
        
        print(f"\n{'='*70}")
        print(f"âœ… Results saved to: {args.out}")
        print(f"{'='*70}\n")
        
        # æ‰“å°æ±‡æ€»è¡¨æ ¼
        print(f"ğŸ“Š SUMMARY TABLE")
        print(f"{'='*70}")
        print(f"{'Model':<30} {'Gran':<8} {'Recall':>8} {'MRR':>8} {'Status':>10}")
        print(f"{'-'*70}")
        
        for r in sorted(results, key=lambda x: (x["recall"], x["mrr"]), reverse=True):
            status = "âœ… PASS" if r["recall"] >= 0.80 and r["mrr"] >= 0.50 else "âŒ FAIL"
            model_short = r["model"].split("/")[-1][:28]
            print(f"{model_short:<30} {r['granularity']:<8} {r['recall']:>8.3f} {r['mrr']:>8.3f} {status:>10}")
        
        print(f"{'='*70}\n")
        
        # æœ€ä½³é…ç½®
        best = max(results, key=lambda x: (x["recall"], x["mrr"]))
        print(f"ğŸ† Best Configuration:")
        print(f"   Model:       {best['model']}")
        print(f"   Granularity: {best['granularity']}")
        print(f"   BM25:        {bool(best['bm25'])}")
        print(f"   Rerank:      {bool(best['rerank'])}")
        print(f"   Recall@{args.k}:   {best['recall']:.3f}")
        print(f"   MRR@{args.k}:      {best['mrr']:.3f}")
        print(f"\nğŸ¯ Target: Recall@10 â‰¥ 0.80, MRR@10 â‰¥ 0.50\n")
        
        # ä¸‹ä¸€æ­¥å»ºè®®
        if best["recall"] < 0.80:
            print(f"ğŸ’¡ Suggestions:")
            print(f"   1. Generate more training data (current: ~{len(test_threads)*5} total)")
            print(f"   2. Try fine-tuning: python train_embedding.py")
            print(f"   3. Use larger model: intfloat/e5-large-v2")

if __name__ == "__main__":
    main()


