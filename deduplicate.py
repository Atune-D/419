#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å»é™¤é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„ threads

æ”¯æŒï¼š
- å®Œå…¨é‡å¤ï¼ˆåŸºäºå†…å®¹å“ˆå¸Œï¼‰
- è¿‘ä¼¼é‡å¤ï¼ˆåŸºäºé¦–æ¡é‚®ä»¶çš„æ–‡æœ¬ç›¸ä¼¼åº¦ï¼‰
- æŒ‰ä¸»é¢˜+è®¢å•IDå»é‡

Usage:
    # åŸºäºå“ˆå¸Œå»é‡ï¼ˆå¿«é€Ÿï¼‰
    python deduplicate.py --input output/threads_*.jsonl --output data/raw/threads_dedup.jsonl
    
    # åŸºäºç›¸ä¼¼åº¦å»é‡ï¼ˆæ›´å‡†ç¡®ä½†æ…¢ï¼‰
    python deduplicate.py --input output/threads_*.jsonl --output data/raw/threads_dedup.jsonl --similarity 0.95
    
    # ä»æ ‡å‡†è¾“å…¥è¯»å–
    cat output/threads_*.jsonl | python deduplicate.py > data/raw/threads.jsonl
"""

import argparse
import jsonlines
import hashlib
import sys
from pathlib import Path
from collections import defaultdict
import glob

def compute_hash(thread):
    """
    è®¡ç®— thread çš„å“ˆå¸Œå€¼ï¼ˆç”¨äºç²¾ç¡®å»é‡ï¼‰
    åŸºäºæ‰€æœ‰ turn çš„ body
    """
    bodies = []
    for turn in thread.get("turns", []):
        body = turn.get("body", "").strip()
        if body:
            bodies.append(body)
    
    content = "\n".join(bodies)
    return hashlib.md5(content.encode()).hexdigest()

def compute_first_turn_hash(thread):
    """
    è®¡ç®—é¦–æ¡é‚®ä»¶çš„å“ˆå¸Œï¼ˆç”¨äºç²—ç²’åº¦å»é‡ï¼‰
    """
    if thread.get("turns") and len(thread["turns"]) > 0:
        first_turn = thread["turns"][0]
        body = first_turn.get("body", "").strip()
        return hashlib.md5(body.encode()).hexdigest()
    return None

def get_metadata_key(thread):
    """
    è·å–å…ƒæ•°æ®é”®ï¼ˆä¸»é¢˜ + è®¢å•IDï¼‰
    """
    topic = thread.get("topic", "unknown")
    order_id = thread.get("meta", {}).get("order_id", "")
    return f"{topic}_{order_id}"

def deduplicate_exact(threads):
    """
    ç²¾ç¡®å»é‡ï¼šå®Œå…¨ç›¸åŒçš„ threads
    """
    seen = set()
    unique = []
    duplicates = 0
    
    for thread in threads:
        h = compute_hash(thread)
        if h not in seen:
            seen.add(h)
            unique.append(thread)
        else:
            duplicates += 1
    
    print(f"  Exact dedup: {len(threads)} â†’ {len(unique)} ({duplicates} duplicates removed)")
    return unique

def deduplicate_first_turn(threads):
    """
    é¦–æ¡é‚®ä»¶å»é‡ï¼šé¦–æ¡é‚®ä»¶ç›¸åŒçš„è§†ä¸ºé‡å¤
    """
    seen = set()
    unique = []
    duplicates = 0
    
    for thread in threads:
        h = compute_first_turn_hash(thread)
        if h and h not in seen:
            seen.add(h)
            unique.append(thread)
        elif not h:
            # æ²¡æœ‰ turn çš„ä¹Ÿä¿ç•™
            unique.append(thread)
        else:
            duplicates += 1
    
    print(f"  First-turn dedup: {len(threads)} â†’ {len(unique)} ({duplicates} duplicates removed)")
    return unique

def deduplicate_by_metadata(threads):
    """
    åŸºäºå…ƒæ•°æ®å»é‡ï¼šç›¸åŒä¸»é¢˜+è®¢å•IDçš„ä¿ç•™æœ€é•¿çš„ä¸€ä¸ª
    """
    groups = defaultdict(list)
    
    for thread in threads:
        key = get_metadata_key(thread)
        groups[key].append(thread)
    
    unique = []
    duplicates = 0
    
    for key, group in groups.items():
        if len(group) == 1:
            unique.append(group[0])
        else:
            # ä¿ç•™ turns æœ€å¤šçš„
            best = max(group, key=lambda t: len(t.get("turns", [])))
            unique.append(best)
            duplicates += len(group) - 1
    
    print(f"  Metadata dedup: {len(threads)} â†’ {len(unique)} ({duplicates} duplicates removed)")
    return unique

def deduplicate_similarity(threads, threshold=0.95):
    """
    åŸºäºç›¸ä¼¼åº¦å»é‡ï¼ˆä½¿ç”¨ç®€å•çš„ Jaccard ç›¸ä¼¼åº¦ï¼‰
    æ³¨æ„ï¼šè¿™ä¸ªæ–¹æ³•è¾ƒæ…¢ï¼Œé€‚ç”¨äºå°è§„æ¨¡æ•°æ®
    """
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
        import numpy as np
    except ImportError:
        print("  âš ï¸  sklearn not installed, skipping similarity dedup")
        return threads
    
    # æå–æ–‡æœ¬
    texts = []
    for thread in threads:
        bodies = []
        for turn in thread.get("turns", []):
            body = turn.get("body", "").strip()
            if body:
                bodies.append(body)
        texts.append(" ".join(bodies))
    
    if not texts:
        return threads
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    print(f"  Computing similarity matrix for {len(texts)} threads...")
    vectorizer = TfidfVectorizer(max_features=1000)
    tfidf = vectorizer.fit_transform(texts)
    similarity_matrix = cosine_similarity(tfidf)
    
    # å»é‡
    keep = [True] * len(threads)
    duplicates = 0
    
    for i in range(len(threads)):
        if not keep[i]:
            continue
        for j in range(i + 1, len(threads)):
            if not keep[j]:
                continue
            if similarity_matrix[i, j] >= threshold:
                # ä¿ç•™ turns æ›´å¤šçš„
                if len(threads[i].get("turns", [])) >= len(threads[j].get("turns", [])):
                    keep[j] = False
                else:
                    keep[i] = False
                    break
                duplicates += 1
    
    unique = [t for i, t in enumerate(threads) if keep[i]]
    print(f"  Similarity dedup (threshold={threshold}): {len(threads)} â†’ {len(unique)} ({duplicates} duplicates removed)")
    return unique

def main():
    parser = argparse.ArgumentParser(description="Deduplicate threads")
    parser.add_argument("--input", nargs="*", help="Input JSONL file(s) (supports glob patterns)")
    parser.add_argument("--output", help="Output JSONL file (if not provided, writes to stdout)")
    parser.add_argument("--method", default="all", 
                       choices=["exact", "first_turn", "metadata", "similarity", "all"],
                       help="Deduplication method")
    parser.add_argument("--similarity", type=float, default=0.95,
                       help="Similarity threshold for similarity dedup (0.0-1.0)")
    args = parser.parse_args()
    
    # è¯»å–è¾“å…¥
    threads = []
    
    if args.input:
        # ä»æ–‡ä»¶è¯»å–
        files = []
        for pattern in args.input:
            files.extend(glob.glob(pattern))
        
        if not files:
            print("âŒ No input files found!")
            return
        
        print(f"ğŸ“‚ Reading from {len(files)} file(s):")
        for f in files:
            print(f"  - {f}")
        
        for f in files:
            try:
                with jsonlines.open(f) as reader:
                    threads.extend(list(reader))
            except Exception as e:
                print(f"  âš ï¸  Error reading {f}: {e}")
    else:
        # ä»æ ‡å‡†è¾“å…¥è¯»å–
        print(f"ğŸ“‚ Reading from stdin...")
        try:
            with jsonlines.Reader(sys.stdin) as reader:
                threads = list(reader)
        except Exception as e:
            print(f"âŒ Error reading stdin: {e}")
            return
    
    if not threads:
        print("âŒ No threads found!")
        return
    
    print(f"âœ… Loaded {len(threads)} threads\n")
    
    # å»é‡
    print(f"ğŸ”„ Deduplicating (method: {args.method})...")
    
    if args.method == "exact" or args.method == "all":
        threads = deduplicate_exact(threads)
    
    if args.method == "first_turn" or args.method == "all":
        threads = deduplicate_first_turn(threads)
    
    if args.method == "metadata" or args.method == "all":
        threads = deduplicate_by_metadata(threads)
    
    if args.method == "similarity":
        threads = deduplicate_similarity(threads, threshold=args.similarity)
    
    print(f"\nâœ… Final: {len(threads)} unique threads\n")
    
    # è¾“å‡º
    if args.output:
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with jsonlines.open(output_path, "w") as writer:
            for thread in threads:
                writer.write(thread)
        
        print(f"âœ… Saved to: {args.output}")
    else:
        # è¾“å‡ºåˆ°æ ‡å‡†è¾“å‡º
        with jsonlines.Writer(sys.stdout) as writer:
            for thread in threads:
                writer.write(thread)

if __name__ == "__main__":
    main()


