#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆæ£€ç´¢è¯„ä¼°è„šæœ¬ - æ”¯æŒå¤šç§ä¼˜åŒ–ç­–ç•¥

âš ï¸  IMPORTANT: ä¸ºé¿å…æ•°æ®æ³„éœ²ï¼Œè¯·ä½¿ç”¨ --train æä¾›ç‹¬ç«‹çš„è®­ç»ƒé›†ç”¨äºæ„å»ºç´¢å¼•ï¼

Usage examples:
  # âœ… æ­£ç¡®ç”¨æ³•ï¼šåˆ†åˆ«æä¾›è®­ç»ƒé›†å’Œæµ‹è¯•é›†
  python eval_retrieval_enhanced.py \
    --train threads.train.jsonl \
    --test threads.test.jsonl \
    --k 10 --compare-all
  
  # âŒ ä¸æ¨èï¼šä»…ç”¨æµ‹è¯•é›†ï¼ˆæ•°æ®æ³„éœ²ï¼Œç»“æœè™šé«˜ï¼‰
  python eval_retrieval_enhanced.py --test threads.test.jsonl --k 10
  
  # ä½¿ç”¨æ›´å¼ºæ¨¡å‹
  python eval_retrieval_enhanced.py \
    --train threads.train.jsonl --test threads.test.jsonl \
    --k 10 --model e5-base-v2
  
  # Turnçº§åˆ‡åˆ†
  python eval_retrieval_enhanced.py \
    --train threads.train.jsonl --test threads.test.jsonl \
    --k 10 --turn-level
  
  # åŠ é‡æ’
  python eval_retrieval_enhanced.py \
    --train threads.train.jsonl --test threads.test.jsonl \
    --k 10 --rerank
  
  # å…¨å¼€
  python eval_retrieval_enhanced.py \
    --train threads.train.jsonl --test threads.test.jsonl \
    --k 10 --model e5-base-v2 --turn-level --rerank
"""

import argparse, jsonlines, re, numpy as np
from tqdm import tqdm
from pathlib import Path
import time

def tokenize(s): 
    return re.findall(r"[a-z0-9]+", (s or "").lower())

def last_by_role(turns, role):
    for t in reversed(turns):
        if t.get("role")==role: 
            return t
    return None

def build_docs_thread_level(threads):
    """åŸç‰ˆï¼šæ¯ä¸ª thread ä½œä¸ºä¸€ä¸ªæ–‡æ¡£"""
    docs, ids = [], []
    for th in threads:
        scsa_texts = []
        for t in th["turns"]:
            sc = t.get("scsa")
            if isinstance(sc, str): 
                scsa_texts.append(sc)
        scsa = "\n".join(scsa_texts)
        
        parts = []
        for t in th["turns"][:4]:
            parts.append(f"[{t.get('role','').upper()}] {t.get('subject','')}\n{t.get('body','')}")
        raw = "\n\n".join(parts)
        text = (scsa.strip() + "\n" + raw.strip()).strip()
        docs.append(text)
        ids.append(th["thread_id"])
    return ids, docs

def build_docs_turn_level(threads):
    """ä¼˜åŒ–ï¼šæ¯ä¸ª turn ä½œä¸ºä¸€ä¸ªæ–‡æ¡£ï¼ˆæ›´ç»†ç²’åº¦æ£€ç´¢ï¼‰"""
    docs, ids = [], []
    for th in threads:
        for i, t in enumerate(th["turns"]):
            role = t.get("role", "")
            subj = t.get("subject", "") or ""
            body = t.get("body", "") or ""
            scsa = t.get("scsa") if isinstance(t.get("scsa"), str) else ""
            
            # æ¯ä¸ª turn åšæˆä¸€ä¸ª docï¼ˆSCSA ä¼˜å…ˆ + RAWï¼‰
            text = (scsa + "\n" + f"[{role.upper()}] {subj}\n{body}").strip()
            docs.append(text)
            ids.append(f"{th['thread_id']}#t{i}")  # ä¿ç•™ turn ä½ç½®ä¿¡æ¯
    return ids, docs

def extract_thread_id(doc_id):
    """ä»æ–‡æ¡£IDä¸­æå–thread_idï¼ˆå¤„ç† turn-level çš„æƒ…å†µï¼‰"""
    if "#t" in doc_id:
        return doc_id.split("#t")[0]
    return doc_id

def load_embedding_model(model_name):
    """åŠ è½½åµŒå…¥æ¨¡å‹"""
    from sentence_transformers import SentenceTransformer
    
    model_map = {
        "minilm": "sentence-transformers/all-MiniLM-L6-v2",  # åŸç‰ˆï¼ˆå¿«ä½†å¼±ï¼‰
        "e5-base-v2": "intfloat/e5-base-v2",                 # æ›´å¼ºçš„é€šç”¨æ£€ç´¢
        "bge-small": "BAAI/bge-small-en-v1.5",               # BGE ç³»åˆ—
        "e5-large": "intfloat/e5-large-v2",                  # æœ€å¼ºï¼ˆä½†æ…¢ï¼‰
    }
    
    model_path = model_map.get(model_name, model_name)
    print(f"ğŸ“¦ Loading embedding model: {model_path}")
    model = SentenceTransformer(model_path)
    return model

def rerank_with_cross_encoder(query, candidates, top_k=10):
    """ä½¿ç”¨äº¤å‰ç¼–ç å™¨é‡æ’"""
    from sentence_transformers import CrossEncoder
    
    reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
    
    # å‡†å¤‡ (query, doc) å¯¹
    pairs = [(query, doc_text) for doc_id, doc_text in candidates]
    
    # æ‰“åˆ†å¹¶é‡æ’
    scores = reranker.predict(pairs)
    reranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)
    
    # è¿”å› top_k ä¸ªæ–‡æ¡£ID
    top_ids = [doc_id for (doc_id, _text), _score in reranked[:top_k]]
    return top_ids

def evaluate(test_threads, doc_ids, doc_texts, model, k=10, use_rerank=False):
    """æ‰§è¡Œæ£€ç´¢è¯„ä¼°"""
    import faiss
    
    # æ„å»º FAISS ç´¢å¼•
    print("ğŸ”¨ Building FAISS index...")
    E_docs = model.encode(
        doc_texts, 
        batch_size=256, 
        normalize_embeddings=True, 
        convert_to_numpy=True, 
        show_progress_bar=True
    )
    dim = E_docs.shape[1]
    index = faiss.IndexFlatIP(dim)
    index.add(E_docs)
    
    # è¯„ä¼°æŒ‡æ ‡
    K = k
    hit = 0
    rr_sum = 0.0
    total = 0
    
    # å¦‚æœä½¿ç”¨é‡æ’ï¼Œå…ˆæ£€ç´¢æ›´å¤šå€™é€‰
    retrieve_k = 100 if use_rerank else K
    
    print(f"ğŸ” Evaluating on {len(test_threads)} test threads...")
    for th in tqdm(test_threads, desc="Evaluating"):
        # æŸ¥è¯¢ = æœ€åä¸€æ¡å®¢æˆ·é‚®ä»¶ï¼šä¼˜å…ˆSCSAï¼Œå¦åˆ™BODY
        q_turn = last_by_role(th["turns"], "customer")
        if not q_turn: 
            continue
        
        q = q_turn.get("scsa") if isinstance(q_turn.get("scsa"), str) else q_turn.get("body", "")
        if not q: 
            continue
        
        # å‘é‡æ£€ç´¢
        vq = model.encode([q], normalize_embeddings=True, convert_to_numpy=True)
        D, I = index.search(vq, retrieve_k)
        
        # æ˜¯å¦ä½¿ç”¨é‡æ’
        if use_rerank:
            candidates = [(doc_ids[i], doc_texts[i]) for i in I[0]]
            top_ids = rerank_with_cross_encoder(q, candidates, top_k=K)
        else:
            top_ids = [doc_ids[i] for i in I[0][:K]]
        
        # æå– thread_idï¼ˆå¤„ç† turn-level çš„æƒ…å†µï¼‰
        top_thread_ids = [extract_thread_id(doc_id) for doc_id in top_ids]
        
        total += 1
        # Recall@K
        if th["thread_id"] in top_thread_ids:
            hit += 1
            # MRR@K
            rank = top_thread_ids.index(th["thread_id"]) + 1
            rr_sum += 1.0 / rank
    
    recall_k = hit / total if total else 0.0
    mrr_k = rr_sum / total if total else 0.0
    
    return {
        "total": total,
        "recall": recall_k,
        "mrr": mrr_k,
        "k": K
    }

def run_evaluation(test_path, model_name="minilm", k=10, turn_level=False, use_rerank=False, train_path=None):
    """è¿è¡Œå•æ¬¡è¯„ä¼°
    
    Args:
        test_path: æµ‹è¯•é›†è·¯å¾„ï¼ˆç”¨äºæŸ¥è¯¢ï¼‰
        train_path: è®­ç»ƒé›†è·¯å¾„ï¼ˆç”¨äºæ„å»ºç´¢å¼•ï¼‰ã€‚å¦‚æœä¸ºNoneï¼Œåˆ™ç”¨æµ‹è¯•é›†æ„å»ºç´¢å¼•ï¼ˆâš ï¸ æ•°æ®æ³„éœ²ï¼ï¼‰
    """
    # è¯»å–æµ‹è¯•é›†
    print(f"\n{'='*60}")
    print(f"ğŸ“Š Configuration:")
    print(f"  Model: {model_name}")
    print(f"  Doc Level: {'Turn' if turn_level else 'Thread'}")
    print(f"  Reranking: {'Yes' if use_rerank else 'No'}")
    print(f"  K: {k}")
    print(f"{'='*60}\n")
    
    test_threads = list(jsonlines.open(test_path))
    assert len(test_threads) > 0, "empty test set"
    
    # å†³å®šç”¨ä»€ä¹ˆæ•°æ®æ„å»ºç´¢å¼•
    if train_path:
        print(f"ğŸ“š Loading train set for index: {train_path}")
        index_threads = list(jsonlines.open(train_path))
        print(f"   Index: {len(index_threads)} threads")
        print(f"   Query: {len(test_threads)} threads")
    else:
        print(f"âš ï¸  WARNING: No train set provided!")
        print(f"   Using test set for BOTH index and queries (DATA LEAKAGE!)")
        print(f"   Results will be artificially high and NOT reliable!")
        print(f"   Use --train flag to provide separate training data.\n")
        index_threads = test_threads
    
    # æ„å»ºæ–‡æ¡£ï¼ˆç”¨äºç´¢å¼•ï¼‰
    if turn_level:
        doc_ids, doc_texts = build_docs_turn_level(index_threads)
    else:
        doc_ids, doc_texts = build_docs_thread_level(index_threads)
    
    print(f"ğŸ”¨ Building index with {len(doc_texts)} documents from {len(index_threads)} threads")
    
    # åŠ è½½æ¨¡å‹
    model = load_embedding_model(model_name)
    
    # è¯„ä¼°
    start_time = time.time()
    results = evaluate(test_threads, doc_ids, doc_texts, model, k=k, use_rerank=use_rerank)
    elapsed = time.time() - start_time
    
    # è¾“å‡ºç»“æœ
    print(f"\n{'='*60}")
    print(f"ğŸ“ˆ Results:")
    print(f"  Total queries: {results['total']}")
    print(f"  Recall@{results['k']}: {results['recall']:.3f}")
    print(f"  MRR@{results['k']}: {results['mrr']:.3f}")
    print(f"  Time: {elapsed:.1f}s")
    print(f"{'='*60}\n")
    
    return results

def compare_all_configs(test_path, k=10, train_path=None):
    """å¯¹æ¯”æ‰€æœ‰é…ç½®ç»„åˆ"""
    print("\n" + "="*70)
    print("ğŸš€ COMPREHENSIVE COMPARISON - Running all configurations...")
    print("="*70)
    
    if not train_path:
        print("\nâš ï¸  " + "="*66)
        print("âš ï¸  WARNING: No separate train set! Using test set for indexing too!")
        print("âš ï¸  This causes DATA LEAKAGE - results will be artificially high!")  
        print("âš ï¸  " + "="*66 + "\n")
    
    configs = [
        # (model_name, turn_level, use_rerank, description)
        ("minilm", False, False, "Baseline (MiniLM + Thread-level)"),
        ("e5-base-v2", False, False, "Better Model (E5-base + Thread-level)"),
        ("minilm", True, False, "Turn-level Split (MiniLM + Turn-level)"),
        ("e5-base-v2", True, False, "Model + Turn-level (E5-base + Turn-level)"),
        ("e5-base-v2", True, True, "Full Optimization (E5-base + Turn-level + Rerank)"),
    ]
    
    results_table = []
    
    for i, (model_name, turn_level, use_rerank, desc) in enumerate(configs, 1):
        print(f"\n{'#'*70}")
        print(f"Config {i}/{len(configs)}: {desc}")
        print(f"{'#'*70}")
        
        try:
            result = run_evaluation(test_path, model_name, k, turn_level, use_rerank, train_path)
            results_table.append({
                "config": desc,
                "model": model_name,
                "turn_level": turn_level,
                "rerank": use_rerank,
                "recall": result["recall"],
                "mrr": result["mrr"]
            })
        except Exception as e:
            print(f"âŒ Error in config {i}: {e}")
            results_table.append({
                "config": desc,
                "model": model_name,
                "turn_level": turn_level,
                "rerank": use_rerank,
                "recall": 0.0,
                "mrr": 0.0
            })
    
    # æ‰“å°æ±‡æ€»è¡¨æ ¼
    print("\n" + "="*100)
    print("ğŸ“Š FINAL COMPARISON TABLE")
    print("="*100)
    print(f"{'Configuration':<50} {'Recall@10':<12} {'MRR@10':<12} {'Status':<15}")
    print("-"*100)
    
    for r in results_table:
        status = "âœ… TARGET MET" if r["recall"] >= 0.80 and r["mrr"] >= 0.50 else "âŒ Below Target"
        print(f"{r['config']:<50} {r['recall']:>10.3f}  {r['mrr']:>10.3f}  {status:<15}")
    
    print("="*100)
    print("\nğŸ¯ Target: Recall@10 â‰¥ 0.80, MRR@10 â‰¥ 0.50")
    
    # æ‰¾å‡ºæœ€ä½³é…ç½®
    best = max(results_table, key=lambda x: (x["recall"], x["mrr"]))
    print(f"\nğŸ† Best Configuration: {best['config']}")
    print(f"   Recall@10: {best['recall']:.3f}")
    print(f"   MRR@10: {best['mrr']:.3f}")
    print("="*100 + "\n")

def main():
    ap = argparse.ArgumentParser(description="Enhanced retrieval evaluation with multiple optimization strategies")
    ap.add_argument("--test", required=True, help="Path to test JSONL file (for queries)")
    ap.add_argument("--train", default=None, help="Path to train JSONL file (for building index). If not provided, uses test set (âš ï¸ DATA LEAKAGE!)")
    ap.add_argument("--k", type=int, default=10, help="K for Recall@K and MRR@K")
    ap.add_argument("--model", default="minilm", 
                    choices=["minilm", "e5-base-v2", "bge-small", "e5-large"],
                    help="Embedding model to use")
    ap.add_argument("--turn-level", action="store_true", 
                    help="Use turn-level documents instead of thread-level")
    ap.add_argument("--rerank", action="store_true", 
                    help="Use cross-encoder reranking")
    ap.add_argument("--compare-all", action="store_true", 
                    help="Run comparison across all configurations")
    args = ap.parse_args()
    
    if args.compare_all:
        compare_all_configs(args.test, k=args.k, train_path=args.train)
    else:
        run_evaluation(args.test, args.model, args.k, args.turn_level, args.rerank, args.train)

if __name__ == "__main__":
    main()

