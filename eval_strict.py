#!/usr/bin/env python3
"""
æ›´ä¸¥æ ¼çš„æ£€ç´¢è¯„ä¼°ï¼šåŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦è€Œä¸æ˜¯ä¸»é¢˜æ ‡ç­¾

è¯„ä¼°æ€è·¯:
1. å¯¹æ¯ä¸ªæŸ¥è¯¢ï¼Œæ‰¾åˆ°"æœ€ç›¸ä¼¼"çš„Kä¸ªæ–‡æ¡£ï¼ˆground truthï¼‰
2. è¯„ä¼°æ¨¡å‹æ£€ç´¢ç»“æœä¸ ground truth çš„é‡å åº¦
3. ä½¿ç”¨æ›´ç»†ç²’åº¦çš„æŒ‡æ ‡ï¼ˆMAP, NDCGï¼‰
"""

import jsonlines
import numpy as np
from tqdm.auto import tqdm
from sentence_transformers import SentenceTransformer
import faiss
from typing import List, Dict, Tuple

def load_threads(path):
    return list(jsonlines.open(path))

def extract_text(thread, granularity="thread"):
    """æå– thread çš„æ–‡æœ¬è¡¨ç¤º"""
    if granularity == "thread":
        texts = []
        for t in thread.get("turns", [])[:4]:
            role = t.get('role', '').upper()
            subj = t.get('subject', '')
            body = t.get('body', '')
            texts.append(f"[{role}] {subj}\n{body}")
        return "\n\n".join(texts)
    else:  # last customer turn
        for t in reversed(thread.get("turns", [])):
            if t.get("role") == "customer":
                return t.get("body", "")
        return ""

def compute_ground_truth(query_threads, index_threads, model_name, top_k=10):
    """
    ä½¿ç”¨å¼ºæ¨¡å‹è®¡ç®—"ç†æƒ³"çš„æ£€ç´¢ç»“æœä½œä¸º ground truth
    
    Args:
        query_threads: æŸ¥è¯¢ threads
        index_threads: ç´¢å¼•åº“ threads  
        model_name: ç”¨äºè®¡ç®—ç›¸ä¼¼åº¦çš„æ¨¡å‹ï¼ˆæœ€å¥½çš„æ¨¡å‹ï¼‰
        top_k: æ¯ä¸ªæŸ¥è¯¢ä¿ç•™å‰Kä¸ªæœ€ç›¸ä¼¼æ–‡æ¡£
    
    Returns:
        ground_truth: {query_idx: [doc_idx1, doc_idx2, ...]}
    """
    print(f"\nğŸ“‹ è®¡ç®— Ground Truthï¼ˆä½¿ç”¨ {model_name.split('/')[-1]}ï¼‰...")
    
    emb = SentenceTransformer(model_name)
    
    # æå–æ–‡æœ¬
    query_texts = [extract_text(th, "thread") for th in query_threads]
    index_texts = [extract_text(th, "thread") for th in index_threads]
    
    # ç¼–ç 
    print("  ç¼–ç æŸ¥è¯¢...")
    query_embs = emb.encode(query_texts, batch_size=32, normalize_embeddings=True, show_progress_bar=True)
    
    print("  ç¼–ç ç´¢å¼•...")
    index_embs = emb.encode(index_texts, batch_size=64, normalize_embeddings=True, show_progress_bar=True)
    
    # è®¡ç®—æ¯ä¸ªæŸ¥è¯¢çš„æœ€ç›¸ä¼¼æ–‡æ¡£
    ground_truth = {}
    
    for i, query_emb in enumerate(tqdm(query_embs, desc="  è®¡ç®—ç›¸ä¼¼åº¦")):
        # ä¸æ‰€æœ‰æ–‡æ¡£çš„ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = np.dot(index_embs, query_emb)
        
        # æ’åºï¼Œå–å‰K
        top_indices = np.argsort(similarities)[::-1][:top_k]
        ground_truth[i] = top_indices.tolist()
    
    return ground_truth

def evaluate_with_ground_truth(
    query_threads, 
    index_threads, 
    model_name, 
    ground_truth, 
    granularity="thread",
    k_values=[5, 10]
):
    """
    åŸºäº ground truth è¯„ä¼°æ¨¡å‹
    
    æŒ‡æ ‡:
    - Recall@K: å‰Kä¸ªä¸­æœ‰å¤šå°‘æ˜¯åœ¨ ground truth ä¸­çš„
    - Precision@K: å‰Kä¸ªä¸­æ­£ç¡®çš„æ¯”ä¾‹
    - MAP@K: Mean Average Precision
    """
    print(f"\n{'='*70}")
    print(f"ğŸ”¬ è¯„ä¼°: {model_name.split('/')[-1]} | {granularity}")
    print(f"{'='*70}")
    
    emb = SentenceTransformer(model_name)
    
    # æ„å»ºç´¢å¼•
    index_texts = [extract_text(th, granularity) for th in index_threads]
    query_texts = [extract_text(th, "thread") for th in query_threads]
    
    print(f"ğŸ“š ç¼–ç  {len(index_texts)} æ–‡æ¡£...")
    index_embs = emb.encode(index_texts, batch_size=64, normalize_embeddings=True, show_progress_bar=True)
    
    # æ„å»º FAISS
    index = faiss.IndexFlatIP(index_embs.shape[1])
    index.add(index_embs)
    
    # è¯„ä¼°
    print(f"ğŸ” è¯„ä¼° {len(query_texts)} æŸ¥è¯¢...")
    
    results = {k: {"recall": [], "precision": [], "ap": []} for k in k_values}
    
    for i, query_text in enumerate(tqdm(query_texts)):
        query_emb = emb.encode([query_text], normalize_embeddings=True)
        
        # æ£€ç´¢
        max_k = max(k_values)
        D, I = index.search(query_emb, max_k)
        retrieved = I[0].tolist()
        
        # Ground truth
        gt = set(ground_truth[i])
        
        # è®¡ç®—æŒ‡æ ‡
        for k in k_values:
            retrieved_k = retrieved[:k]
            hits = [idx for idx in retrieved_k if idx in gt]
            
            # Recall@K
            recall = len(hits) / len(gt) if len(gt) > 0 else 0
            results[k]["recall"].append(recall)
            
            # Precision@K
            precision = len(hits) / k
            results[k]["precision"].append(precision)
            
            # Average Precision@K
            if len(hits) > 0:
                ap = sum((i+1) / (retrieved_k.index(h) + 1) for i, h in enumerate(hits)) / len(gt)
                results[k]["ap"].append(ap)
            else:
                results[k]["ap"].append(0.0)
    
    # æ±‡æ€»
    print(f"\nğŸ“Š ç»“æœ:")
    summary = {}
    for k in k_values:
        recall = np.mean(results[k]["recall"])
        precision = np.mean(results[k]["precision"])
        map_score = np.mean(results[k]["ap"])
        
        print(f"\n  K={k}:")
        print(f"    Recall@{k}:    {recall:.3f}")
        print(f"    Precision@{k}: {precision:.3f}")
        print(f"    MAP@{k}:       {map_score:.3f}")
        
        summary[f"recall@{k}"] = round(recall, 3)
        summary[f"precision@{k}"] = round(precision, 3)
        summary[f"map@{k}"] = round(map_score, 3)
    
    return {
        "model": model_name,
        "granularity": granularity,
        **summary
    }

def main():
    import argparse
    import pandas as pd
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--index", required=True, help="ç´¢å¼•åº“ jsonl æ–‡ä»¶ï¼ˆå¯ä»¥æ˜¯å¤šä¸ªæ–‡ä»¶çš„åˆå¹¶ï¼‰")
    parser.add_argument("--query", required=True, help="æŸ¥è¯¢é›† jsonl æ–‡ä»¶")
    parser.add_argument("--models", nargs="+", default=["intfloat/e5-base-v2"])
    parser.add_argument("--granularities", nargs="+", default=["thread", "turn"])
    parser.add_argument("--k", nargs="+", type=int, default=[5, 10])
    parser.add_argument("--gt-model", default="intfloat/e5-base-v2", help="ç”¨äºè®¡ç®— ground truth çš„æ¨¡å‹")
    args = parser.parse_args()
    
    # åŠ è½½æ•°æ®
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    index_threads = load_threads(args.index)
    query_threads = load_threads(args.query)
    
    print(f"âœ… ç´¢å¼•åº“: {len(index_threads)} threads")
    print(f"âœ… æŸ¥è¯¢é›†: {len(query_threads)} threads")
    
    # è®¡ç®— ground truth
    ground_truth = compute_ground_truth(
        query_threads, 
        index_threads, 
        args.gt_model, 
        top_k=max(args.k)
    )
    
    # è¯„ä¼°æ¯ä¸ªæ¨¡å‹
    results = []
    
    for model_name in args.models:
        for granularity in args.granularities:
            result = evaluate_with_ground_truth(
                query_threads,
                index_threads,
                model_name,
                ground_truth,
                granularity,
                args.k
            )
            results.append(result)
    
    # æ˜¾ç¤ºç»“æœ
    print("\n" + "="*70)
    print("ğŸ“Š æœ€ç»ˆç»“æœ")
    print("="*70)
    
    df = pd.DataFrame(results)
    print(df.to_string(index=False))
    
    # ä¿å­˜
    df.to_csv("results_strict_eval.csv", index=False)
    print("\nâœ… ç»“æœå·²ä¿å­˜: results_strict_eval.csv")

if __name__ == "__main__":
    main()


